# 术语表

## 基本概念

**Prompt（提示）：** Prompt 是用户提供给 AI 模型以引发响应的输入，通常采用问题、指令或陈述的形式。Prompt 的质量和结构极大地影响模型的输出，使 prompt engineering 成为有效使用 AI 的关键技能。

**Context Window（上下文窗口）：** 上下文窗口是 AI 模型一次可以处理的最大 token 数量，包括输入和生成的输出。这个固定大小是一个关键限制，因为窗口外的信息被忽略，而更大的窗口使更复杂的对话和文档分析成为可能。

**In-Context Learning（上下文学习）：** 上下文学习是 AI 从直接在 prompt 中提供的示例中学习新任务的能力，无需任何重新训练。这个强大的功能允许单个通用模型即时适应无数特定任务。

**Zero-Shot, One-Shot, & Few-Shot Prompting（零样本、单样本和少样本提示）：** 这些是提示技术，其中模型被给予零个、一个或几个任务示例来指导其响应。提供更多示例通常有助于模型更好地理解用户的意图并提高其在特定任务上的准确性。

**Multimodality（多模态）：** 多模态是 AI 理解和处理跨多种数据类型（如文本、图像和音频）的信息的能力。这允许更多样化和类似人类的交互，例如描述图像或回答口语问题。

**Grounding（接地）：** Grounding 是将模型的输出连接到可验证的真实世界信息源以确保事实准确性并减少幻觉的过程。这通常通过 RAG 等技术来实现，使 AI 系统更加值得信赖。

## 核心 AI 模型架构

**Transformers（Transformer）：** Transformer 是大多数现代 LLM 的基础神经网络架构。其关键创新是自注意力机制，它高效地处理长文本序列并捕获单词之间的复杂关系。

**Recurrent Neural Network (RNN)（循环神经网络）：** 循环神经网络是 Transformer 之前的基础架构。RNN 顺序处理信息，使用循环来维护先前输入的"记忆"，这使得它们适用于文本和语音处理等任务。

**Mixture of Experts (MoE)（专家混合）：** 专家混合是一种高效的模型架构，其中"路由器"网络动态选择一小部分"专家"网络来处理任何给定的输入。这允许模型拥有大量参数，同时保持计算成本可控。

**Diffusion Models（扩散模型）：** 扩散模型是擅长创建高质量图像的生成模型。它们通过向数据添加随机噪声，然后训练模型仔细地逆转该过程来工作，允许它们从随机起点生成新数据。

**Mamba：** Mamba 是一种使用选择性状态空间模型（SSM）高效处理序列的最近 AI 架构，特别是对于非常长的上下文。其选择性机制允许它专注于相关信息，同时过滤掉噪声，使其成为 Transformer 的潜在替代方案。

## LLM 开发生命周期

强大语言模型的开发遵循一个独特的序列。它从预训练（Pre-training）开始，通过在大规模通用互联网文本数据集上训练来构建大规模基础模型，以学习语言、推理和世界知识。接下来是微调（Fine-tuning），这是一个专业化阶段，通用模型在较小的、特定任务的数据集上进一步训练，以使其能力适应特定目的。最后阶段是对齐（Alignment），调整专业化模型的行为以确保其输出有帮助、无害并与人类价值观保持一致。

**Pre-training Techniques（预训练技术）：** 预训练是模型从大量数据中学习一般知识的初始阶段。此阶段的顶级技术涉及模型学习的不同目标。最常见的是因果语言建模（CLM），其中模型预测句子中的下一个单词。另一个是掩码语言建模（MLM），其中模型填充文本中故意隐藏的单词。其他重要方法包括去噪目标（Denoising Objectives），其中模型学习将损坏的输入恢复到其原始状态；对比学习（Contrastive Learning），其中它学习区分相似和不同的数据片段；以及下一句预测（NSP），其中它确定两个句子是否在逻辑上相互跟随。

**Fine-tuning Techniques（微调技术）：** 微调是使用较小的、专业化的数据集将通用预训练模型适应特定任务的过程。最常见的方法是监督微调（SFT），其中模型在正确输入-输出对的标记示例上训练。一个流行的变体是指令调优（Instruction Tuning），它专注于训练模型以更好地遵循用户命令。为了使这个过程更高效，使用了参数高效微调（PEFT）方法，顶级技术包括 LoRA（低秩适应），它只更新少量参数，以及其内存优化版本 QLoRA。另一种技术，检索增强生成（RAG），通过在微调或推理阶段将模型连接到外部知识源来增强模型。

**Alignment & Safety Techniques（对齐和安全技术）：** 对齐是确保 AI 模型的行为与人类价值观和期望保持一致，使其有帮助和无害的过程。最突出的技术是人类反馈强化学习（RLHF），其中基于人类偏好训练的"奖励模型"指导 AI 的学习过程，通常使用像近端策略优化（PPO）这样的算法来保持稳定性。已经出现了更简单的替代方案，例如直接偏好优化（DPO），它绕过了对单独奖励模型的需求，以及 Kahneman-Tversky 优化（KTO），它进一步简化了数据收集。为了确保安全部署，Guardrails 被实现为最终安全层，以过滤输出并实时阻止有害操作。

## 增强 AI Agent 能力

AI agent 是能够感知其环境并采取自主行动以实现目标的系统。它们的有效性通过强大的推理框架得到增强。

**Chain of Thought (CoT)（思维链）：** 这种提示技术鼓励模型在给出最终答案之前逐步解释其推理。这种"大声思考"的过程通常在复杂推理任务上产生更准确的结果。

**Tree of Thoughts (ToT)（思维树）：** 思维树是一种高级推理框架，其中 agent 同时探索多个推理路径，就像树上的分支。它允许 agent 自我评估不同的思路并选择最有希望的一个来追求，使其在复杂问题解决方面更加有效。

**ReAct (Reason and Act)（推理与行动）：** ReAct 是一个在循环中结合推理和行动的 agent 框架。Agent 首先"思考"要做什么，然后使用工具采取"行动"，并使用产生的观察来通知其下一个想法，使其在解决复杂任务方面非常有效。

**Planning（规划）：** 这是 agent 将高级目标分解为一系列较小的、可管理的子任务的能力。然后，agent 创建一个计划来按顺序执行这些步骤，使其能够处理复杂的多步骤任务。

**Deep Research（深度研究）：** 深度研究是指 agent 通过迭代搜索信息、综合发现和识别新问题来自主深入探索主题的能力。这允许 agent 构建对主题的全面理解，远远超出单个搜索查询。

**Critique Model（批判模型）：** 批判模型是专门训练的 AI 模型，用于审查、评估和提供对另一个 AI 模型输出的反馈。它充当自动化批判者，帮助识别错误、改进推理并确保最终输出达到所需的质量标准。
